# ğŸ¬ RAGify â€“ Civic Video Question Answering Agent

RAGify is an intelligent pipeline for civic meeting video understanding. It transcribes, extracts text from video frames, combines both sources, and enables question-answering using Retrieval-Augmented Generation (RAG).

---

## ğŸš€ Features

- ğŸ§  Transcript-based Q&A using local LLMs (Ollama + Mistral)
- ğŸ–¼ï¸ OCR from video frames (for on-screen data like dates or slides)
- ğŸ§© Chunked transcript + OCR fusion for better context
- ğŸ” Vector-based retrieval with Sentence Transformers + ChromaDB
- ğŸ’¬ Interactive Q&A CLI
- ğŸ“¦ Auto-generated summaries (optional)
- ğŸ”„ Scalable to multiple videos (just drop into `/data/videos`)

---

## ğŸ“ Project Structure
```bash
ragify/
â”‚
â”œâ”€â”€ app/
â”‚ â”œâ”€â”€ run_pipeline.py # Full pipeline (transcribe â†’ ocr â†’ chunk â†’ embed â†’ QA)
â”‚ â”œâ”€â”€ chunker.py # Combines transcript + OCR â†’ JSON chunks
â”‚ â”œâ”€â”€ embedder.py # Embeds chunks into ChromaDB
â”‚ â”œâ”€â”€ ocr_extractor.py # OCR from video frames using EasyOCR
â”‚ â”œâ”€â”€ qa.py # CLI Q&A interface
â”‚ â”œâ”€â”€ retriever.py # Chunk retriever logic
â”‚ â””â”€â”€ assets/ # Logo and static assets
â”‚
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ videos/ # Raw videos (video1.mp4, video2.webm)
â”‚ â”œâ”€â”€ frames/ # Extracted frames from videos
â”‚ â”œâ”€â”€ ocr_texts/ # OCR results (video1.json, ...)
â”‚ â””â”€â”€ transcripts/ # Whisper transcripts (video1.json, ...)
â”‚
â”œâ”€â”€ chunks/ # Final chunked data (video1.json, ...)
â”œâ”€â”€ db/ # ChromaDB vector store
â”œâ”€â”€ embeddings/ # Optional saved embeddings
â”œâ”€â”€ export/ # Exported session summaries & QAs
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

```
## ğŸ”§ Setup
```bash
# 1. Clone the repo
git clone https://github.com/yourusername/ragify.git
cd ragify

# 2. Install dependencies
pip install -r requirements.txt

# 3. (Optional) Download model weights
# If using Ollama:
ollama pull mistral
```
ğŸ“½ï¸ Usage Guide
1. ğŸ“ Add your video
```bash
Place your .mp4 / .webm video in data/videos/ folder and name it like:
video1.mp4
video2.webm
Update VIDEO_ID in run_pipeline.py to match the video filename (without extension).
```
2. â–¶ï¸ Run full pipeline
```bash
python app/run_pipeline.py
```
This will:
- Run Whisper transcription
- Extract frames and apply OCR
- Create transcript + OCR chunks
- Embed into ChromaDB
- Launch the Q&A CLI

3. â“ Ask Questions
```bash
Inside the CLI:
Ask: What did the speaker say about housing policy?
Ask: summary
Ask: exit
```
ğŸ§  Tech Stack:

- Whisper (ASR)
- EasyOCR (image-to-text)
- entenceTransformers (all-MiniLM-L6-v2)
- ChromaDB (vector DB)
- Ollama (local LLMs),
- Python CLI (no frontend yet)

ğŸ“Œ TODO (Upcoming)

1. React frontend like Groq Chat,
2. Admin dashboard for summary & QA logs,
3. Multi-video search,
4. OCR post-processing for better accuracy
